{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import torch.optim as optim                          # optimization\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm                                # for progress bar\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "from clustering import clus\n",
    "\n",
    "from numba import njit\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"running on the gpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"running on the cpu\")\n",
    "    \n",
    "from compression import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"xin\"\n",
    "do_clus = True\n",
    "latent = pipeline(path, 1024, 10, False, 1024, [10, 20], vae_choice='paper', retrain=True) # a list with 3 differently trained latent variables\n",
    "print(len(latent), latent[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b964112",
   "metadata": {},
   "source": [
    "## Dimension reduction and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf305cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "def scDHA_w(data):\n",
    "    K = 3\n",
    "    sample_prob = None\n",
    "    do_clus = True\n",
    "    gen_fil = True\n",
    "    k = None\n",
    "    # if sparse:\n",
    "    #     # don't understand original R code\n",
    "    #     '''\n",
    "    #     if(sparse){\n",
    "    #         if (min(data) < 0) stop(\"The input must be a non negative sparse matrix\")\n",
    "    #         if (max(data) - min(data) > 100){\n",
    "    #             data@x <- log2(data@x + 1) what is data@x ??\n",
    "    #         }\n",
    "    #         data <- normalize_data_sparse(data)\n",
    "    #     }\n",
    "    #     ''' \n",
    "    #     if data.min() < 0:\n",
    "    #         exit(\"The input must be a non negative sparse matrix\")\n",
    "           \n",
    "    #     if data.max() - data.min() > 100:\n",
    "    #         pass\n",
    "    # else:\n",
    "    #     if data.max() - data.min() > 100:\n",
    "    #         if data.min() < 0:\n",
    "    #             data -= data.min(axis=0)\n",
    "    #         data = np.log2(data + 1)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9cb1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4214ba60",
   "metadata": {},
   "source": [
    "## The following code is skipped (wMetaC part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbcf50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if do_clus:\n",
    "    # Use an ensemble of data projection models to achieve higher accuracy and to avoid local minima, not needed if we use kmeans++\n",
    "    # first repeat the data projection\n",
    "    labels = []\n",
    "    for hidden in latent:\n",
    "        # labels.append(clus(hidden, k=6, nmax=100))\n",
    "        labels.append(clus(hidden, nmax=10))\n",
    "    labels = np.array(labels) \n",
    "    print(labels)   \n",
    "    S = np.zeros((len(labels), len(labels)))  # chance that cell i and j are in the same cluster\n",
    "    for i, row in enumerate(S):\n",
    "        for j, _ in enumerate(S):\n",
    "            if not (i==j):\n",
    "                S[i, j] = adjusted_rand_score(labels[i], labels[j])\n",
    "    for i, row in enumerate(S):\n",
    "        S[i,i] = row.mean()\n",
    "    print(S)\n",
    "    found = False\n",
    "    if (S[S < 0.7]).sum() > 0:\n",
    "        i = 2\n",
    "    else:\n",
    "        i = 1\n",
    "        \n",
    "    # find best guessed label (latent variable)\n",
    "    while not found:\n",
    "        # print(f'i={i}')\n",
    "        tmp = KMeans(n_clusters = i, n_init = 100, max_iter = 5000).fit(S)\n",
    "        k = tmp.labels_\n",
    "        max = 0\n",
    "        for c in range(tmp.cluster_centers_.shape[0]): # for k clusters\n",
    "            score = S[k == c, k == c].mean()\n",
    "            if score > max and (k==c).sum() > 1:\n",
    "                max = score\n",
    "                idx = (k == c)\n",
    "        if max > 0.8:\n",
    "            found = True\n",
    "        if i >= 3:\n",
    "            found = True\n",
    "        \n",
    "        i += 1\n",
    "    # guess number of clusters\n",
    "    tmp = []\n",
    "    for label in labels[idx]:\n",
    "        tmp.append(np.unique(label).shape[0])\n",
    "        print(tmp)\n",
    "    cluster_max = np.floor(np.mean(tmp)+0.5).astype(np.int)\n",
    "    \n",
    "    \n",
    "    # (i) calculate cell-cell weighted similarity matrix \n",
    "    W = S * (1 - S)\n",
    "    print(W.max(), W.min())\n",
    "    # then combine the clustering results using the wMetaC\n",
    "    # wMetaC = AgglomerativeClustering(n_clusters=k_classes, linkage='ward')\n",
    "    # # wMetaC = AgglomerativeClustering(n_clusters=k_classes, affinity='precomputed')\n",
    "    # wMetaC.fit(latent)\n",
    "    # # wMetaC.fit(clustered.affinity_matrix_.toarray())\n",
    "    # print(wMetaC)\n",
    "    # print(clustered.labels_)\n",
    "    # print(wMetaC.labels_)\n",
    "\n",
    "        # print(latent.size())\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline('campbell', 1024, 10, False, 1024, [10, 20], vae_choice='paper', retrain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41b6c0752d4e012e3eac71aaaca6904fd15380f04bbe1ef1c7bcd4dfab5ac212"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('pyr': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
