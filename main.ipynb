{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "running on the gpu\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm                                # for progress bar\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, SpectralClustering\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"running on the gpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"running on the cpu\")\n",
    "    \n",
    "from compression import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log2 is applied\n",
      "first column of normalized_data\n",
      "[0.45673727 0.41277488 0.45339973 0.34339266 0.39761435 0.47851649\n",
      " 0.42687017 0.41511023 0.33093438 0.35535198 0.36670658 0.45023024\n",
      " 0.40043435 0.45196978 0.37736807 0.39095186 0.23418182 0.39985241\n",
      " 0.46387343 0.4330633  0.37242285 0.3996168  0.34810324 0.45229824\n",
      " 0.35513868 0.45978068 0.44874997 0.47019561 0.45432799 0.50368729\n",
      " 0.43392305 0.40690908 0.36245137 0.42567683 0.42778335 0.441645\n",
      " 0.49527818 0.47426508 0.45409    0.43216159 0.45311978 0.45456565\n",
      " 0.51180229 0.46854753 0.37801226 0.42012368 0.4831555  0.45191327\n",
      " 0.42939831 0.48954521 0.48692279 0.46442425 0.44307403 0.36137441\n",
      " 0.49888944 0.41050206 0.39993837 0.39971253 0.38087504 0.47462718\n",
      " 0.3897801  0.42853193 0.29358917 0.50614974 0.49537921 0.49197499\n",
      " 0.44618553 0.45329972 0.46797146 0.47187686 0.4864775  0.49805648\n",
      " 0.43143022 0.2958528  0.20408614 0.31025091 0.3421828  0.34659626\n",
      " 0.3576697  0.30695883 0.35534803 0.33608001 0.09025048 0.31171175\n",
      " 0.35811395 0.35291132 0.24383199 0.32395793 0.29860885 0.23308579\n",
      " 0.47457557 0.44834683 0.45638079 0.43851127 0.41769053 0.44012098\n",
      " 0.41706994 0.37431248 0.44159398 0.4637505  0.35753507 0.484384\n",
      " 0.46498891 0.41727993 0.42192937 0.43329739 0.41723639 0.36954299\n",
      " 0.35923993 0.31234759 0.43200371 0.4771033  0.49025638 0.38951079\n",
      " 0.40928951 0.47266503 0.34939679 0.32856178 0.38421194 0.36671228\n",
      " 0.43535964 0.40658131 0.35168539 0.39614608]\n",
      "non_negative_kernel_autoencoder(\n",
      "  (encoder): Linear(in_features=41428, out_features=50, bias=True)\n",
      "  (decoder): Linear(in_features=50, out_features=41428, bias=True)\n",
      ")\n",
      "training non-negative kernel autoencoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 92.94it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 286.48it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 286.48it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 287.81it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 297.45it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 291.86it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 280.16it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 303.25it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 289.14it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 283.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 Loss:  0.19321280717849731\n",
      "paper_encoder(\n",
      "  (encoder): Linear(in_features=124, out_features=64, bias=True)\n",
      "  (bn): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (mu): Linear(in_features=64, out_features=15, bias=True)\n",
      "  (var): Linear(in_features=64, out_features=15, bias=True)\n",
      "  (sample_expander): ModuleList(\n",
      "    (0): Linear(in_features=15, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=15, out_features=64, bias=True)\n",
      "  )\n",
      "  (decoder): ModuleList(\n",
      "    (0): Linear(in_features=64, out_features=124, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=124, bias=True)\n",
      "  )\n",
      ")\n",
      "training stacked bayesian autoencoder\n",
      "\n",
      "###############################\n",
      "#phase 1: the warm-up process#\n",
      "######################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 129.50it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 150.15it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 134.56it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 149.35it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 145.11it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 141.77it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 131.99it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 131.88it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 142.25it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 133.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 Loss: 0.03833863139152527\n",
      "\n",
      "##############################\n",
      "#phase 2: the VAE stage#\n",
      "############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 111.81it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 123.72it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 106.27it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 121.15it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 128.93it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 127.39it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 113.52it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 123.62it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 120.71it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 128.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 Loss: 1.8543415069580078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 117.49it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 117.07it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 116.85it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 122.58it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 115.44it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 117.74it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 119.77it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 123.52it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 113.65it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 101.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 Loss: 1.8637399673461914\n",
      "[[ 0.41278243  0.14169373 -0.46996102 ...  0.05423163  0.04568342\n",
      "   0.00834418]\n",
      " [ 0.4416948   0.22213012 -0.4397324  ...  0.02734448 -0.02954184\n",
      "   0.11566014]\n",
      " [ 0.02712245  0.01052719 -0.08650968 ...  0.00842969  0.01560827\n",
      "   0.01188319]\n",
      " ...\n",
      " [ 0.02712245  0.01052719 -0.08650968 ...  0.00842969  0.01560827\n",
      "   0.01188319]\n",
      " [-0.17385882  0.00830169 -0.6941473  ... -0.5200063   0.21937646\n",
      "   0.31812555]\n",
      " [ 0.30791172 -0.18086055 -0.12965988 ... -0.5453196   0.18314217\n",
      "   0.45523423]] 1.3928463 -1.3364081\n",
      "124 (15,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"goolam\"\n",
    "do_clus = True\n",
    "latent = pipeline(path_name=path, is_plot_denoise=False, vae_choice='paper', retrain=True) # a list with 3 differently trained latent variables\n",
    "print(len(latent), latent[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40e9eb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 4 4 4 2 3 3 0 4 4 3 2 3 4 1 1 2 1 2 4 4 4 2 2 3 0 2 4 2 4 4 3 4 3 4 1\n",
      " 1 4 0 4 2 4 3 0 4 2 4 3 1 3 3 4 3 3 4 4 4 3 4 1 4 3 1 1 0 4] [0 3 4 4 2 4 3 3 2 5 1 0 4 0 5 0 0 4 3 4 4 1 4 4 4 3 2 4 0 4 0 4 0 4 3 4 3\n",
      " 3 4 2 2 3 2 0 2 1 3 4 3 4 0 3 4 4 0 5 4 1 3 1 0 3 3 0 3 3 4]\n",
      "0.16990115697659067\n"
     ]
    }
   ],
   "source": [
    "clustering = SpectralClustering(5, eigen_solver='arpack', affinity='nearest_neighbors', n_neighbors=7,  \n",
    "                                                        assign_labels='kmeans', n_jobs=-1, random_state=0)  # n_jobs = -1 means use all processors    \n",
    "clustering.fit(X_train)\n",
    "print(clustering.labels_, y_train)\n",
    "score = adjusted_rand_score(clustering.labels_, y_train)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3cee30",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a70fd9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4cell' 'blast' '4cell' '4cell' '8cell' '4cell' '4cell' '8cell' '4cell'\n",
      " '8cell' '4cell' '8cell' '4cell' '2cell' '4cell' '2cell' '4cell' '4cell'\n",
      " '8cell' '8cell' '8cell' '8cell' 'blast' '8cell' '4cell' '4cell' '4cell'\n",
      " '4cell' '4cell' '4cell' '4cell']\n",
      "['2cell' '4cell' '4cell' '4cell' '4cell' '4cell' '4cell' '4cell' '4cell'\n",
      " '8cell' '4cell' '8cell' '4cell' '2cell' '4cell' '4cell' '2cell' '4cell'\n",
      " '4cell' '4cell' '4cell' '16cell' '4cell' '8cell' '2cell' '4cell' '4cell'\n",
      " '4cell' '4cell' '4cell' '4cell']\n",
      "0.12098298676748583\n"
     ]
    }
   ],
   "source": [
    "path_name = 'goolam'\n",
    "\n",
    "X = np.load(f'latent/{path_name}.npy')\n",
    "# print(X)\n",
    "y_df = pd.read_csv(f'rds_csv_data/{path_name}_labels.csv')['x']\n",
    "y = np.array(y_df)\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# y = le.fit_transform(y)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from scipy.spatial.distance import correlation\n",
    "neigh = KNeighborsClassifier(n_neighbors=5, metric=correlation, n_jobs=-1)\n",
    "neigh.fit(X_train, y_train)\n",
    "pred_y = neigh.predict(X_test)\n",
    "print(y_test)\n",
    "print(pred_y)\n",
    "score = adjusted_rand_score(pred_y, y_test)\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b964112",
   "metadata": {},
   "source": [
    "## Dimension reduction and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf305cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scDHA_w(data):\n",
    "    K = 3\n",
    "    sample_prob = None\n",
    "    do_clus = True\n",
    "    gen_fil = True\n",
    "    k = None\n",
    "    # if sparse:\n",
    "    #     # don't understand original R code\n",
    "    #     '''\n",
    "    #     if(sparse){\n",
    "    #         if (min(data) < 0) stop(\"The input must be a non negative sparse matrix\")\n",
    "    #         if (max(data) - min(data) > 100){\n",
    "    #             data@x <- log2(data@x + 1) what is data@x ??\n",
    "    #         }\n",
    "    #         data <- normalize_data_sparse(data)\n",
    "    #     }\n",
    "    #     ''' \n",
    "    #     if data.min() < 0:\n",
    "    #         exit(\"The input must be a non negative sparse matrix\")\n",
    "           \n",
    "    #     if data.max() - data.min() > 100:\n",
    "    #         pass\n",
    "    # else:\n",
    "    #     if data.max() - data.min() > 100:\n",
    "    #         if data.min() < 0:\n",
    "    #             data -= data.min(axis=0)\n",
    "    #         data = np.log2(data + 1)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4214ba60",
   "metadata": {},
   "source": [
    "## The following code is skipped (wMetaC part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbcf50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if do_clus:\n",
    "    # Use an ensemble of data projection models to achieve higher accuracy and to avoid local minima, not needed if we use kmeans++\n",
    "    # first repeat the data projection\n",
    "    labels = []\n",
    "    for hidden in latent:\n",
    "        # labels.append(clus(hidden, k=6, nmax=100))\n",
    "        labels.append(clus(hidden, nmax=10))\n",
    "    labels = np.array(labels) \n",
    "    print(labels)   \n",
    "    S = np.zeros((len(labels), len(labels)))  # chance that cell i and j are in the same cluster\n",
    "    for i, row in enumerate(S):\n",
    "        for j, _ in enumerate(S):\n",
    "            if not (i==j):\n",
    "                S[i, j] = adjusted_rand_score(labels[i], labels[j])\n",
    "    for i, row in enumerate(S):\n",
    "        S[i,i] = row.mean()\n",
    "    print(S)\n",
    "    found = False\n",
    "    if (S[S < 0.7]).sum() > 0:\n",
    "        i = 2\n",
    "    else:\n",
    "        i = 1\n",
    "        \n",
    "    # find best guessed label (latent variable)\n",
    "    while not found:\n",
    "        # print(f'i={i}')\n",
    "        tmp = KMeans(n_clusters = i, n_init = 100, max_iter = 5000).fit(S)\n",
    "        k = tmp.labels_\n",
    "        max = 0\n",
    "        for c in range(tmp.cluster_centers_.shape[0]): # for k clusters\n",
    "            score = S[k == c, k == c].mean()\n",
    "            if score > max and (k==c).sum() > 1:\n",
    "                max = score\n",
    "                idx = (k == c)\n",
    "        if max > 0.8:\n",
    "            found = True\n",
    "        if i >= 3:\n",
    "            found = True\n",
    "        \n",
    "        i += 1\n",
    "    # guess number of clusters\n",
    "    tmp = []\n",
    "    for label in labels[idx]:\n",
    "        tmp.append(np.unique(label).shape[0])\n",
    "        print(tmp)\n",
    "    cluster_max = np.floor(np.mean(tmp)+0.5).astype(np.int)\n",
    "    \n",
    "    \n",
    "    # (i) calculate cell-cell weighted similarity matrix \n",
    "    W = S * (1 - S)\n",
    "    print(W.max(), W.min())\n",
    "    # then combine the clustering results using the wMetaC\n",
    "    # wMetaC = AgglomerativeClustering(n_clusters=k_classes, linkage='ward')\n",
    "    # # wMetaC = AgglomerativeClustering(n_clusters=k_classes, affinity='precomputed')\n",
    "    # wMetaC.fit(latent)\n",
    "    # # wMetaC.fit(clustered.affinity_matrix_.toarray())\n",
    "    # print(wMetaC)\n",
    "    # print(clustered.labels_)\n",
    "    # print(wMetaC.labels_)\n",
    "\n",
    "        # print(latent.size())\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline('campbell', 1024, 10, False, 1024, [10, 20], vae_choice='paper', retrain=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41b6c0752d4e012e3eac71aaaca6904fd15380f04bbe1ef1c7bcd4dfab5ac212"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('pyr': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
